{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buidling Tenserflow version of the Readability Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and take `READABILITY_SCORES` and `POS_DENSITY` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../data')\n",
    "from corpus import load_corpus\n",
    "data = load_corpus('weebit')\n",
    "\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "y_train_onehot = data['y_train_onehot']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "y_test_onehot = data['y_test_onehot']\n",
    "\n",
    "FEATURES_NAMES = data['FEATURES_NAMES']\n",
    "features = FEATURES_NAMES['READABILITY_SCORES'] + FEATURES_NAMES['POS_DENSITY']\n",
    "\n",
    "X_train = X_train[features]\n",
    "X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def threshold_socre(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred) <= 1) / len(y_true)\n",
    "\n",
    "def calc_scores(y_true, y_pred_probs):\n",
    "    y_pred_avg = (y_pred_probs * np.arange(5)).sum(axis=1)\n",
    "    y_pred_avg_classes = y_pred_avg.round().clip(0, 4).astype(int)\n",
    "    return {'accuracy': accuracy_score(y_true, y_pred_avg_classes),\n",
    "            'threshold': threshold_socre(y_true, y_pred_avg_classes)}\n",
    "\n",
    "threshold_scorer = make_scorer(threshold_socre)\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "def calc_scores_with_cv(model, X, y, cv=5):\n",
    "    return {'accuracy': np.mean(cross_val_score(model, X, y, scoring=accuracy_scorer, cv=cv)),\n",
    "            'threshold': np.mean(cross_val_score(model, X, y, scoring=threshold_scorer, cv=cv))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Softmax layer for the SVM Probabilites Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: {'threshold': 0.9068099410863921, 'accuracy': 0.6990836779451828}\n",
      "Test: {'threshold': 0.929945054945055, 'accuracy': 0.6167582417582418}\n",
      "Test - as normal classifier accuracy: 0.7060439560439561\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_ovo = SVC(kernel='rbf', C=1, probability=True, decision_function_shape='ovo')\n",
    "\n",
    "print('CV:', calc_scores_with_cv(model_ovo, X_train, y_train))\n",
    "\n",
    "model_ovo.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_ovo.predict(X_test)\n",
    "y_pred_probs = model_ovo.predict_proba(X_test)\n",
    "print('Test:', calc_scores(y_test, y_pred_probs))\n",
    "print('Test - as normal classifier accuracy:', model_ovo.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: {'threshold': 0.9360375226975648, 'accuracy': 0.779224112946632}\n",
      "Test: {'threshold': 0.9285714285714286, 'accuracy': 0.6126373626373627}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "def create_prob_calculator_ovo():\n",
    "    prob_calculator_ovo = Sequential()\n",
    "    prob_calculator_ovo.add(Dense(5, input_dim=10, activation='softmax'))\n",
    "\n",
    "    prob_calculator_ovo.compile(optimizer='rmsprop',\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "    return prob_calculator_ovo\n",
    "\n",
    "\n",
    "prob_calculator_ovo = KerasClassifier(build_fn=create_prob_calculator_ovo,\n",
    "                                      epochs=10, \n",
    "                                      verbose=0)\n",
    "\n",
    "\n",
    "X_dist_ovo_train = model_ovo.decision_function(X_train)\n",
    "X_dist_ovo_test = model_ovo.decision_function(X_test)\n",
    "\n",
    "print('CV:', calc_scores_with_cv(prob_calculator_ovo, X_dist_ovo_train, y_train))\n",
    "\n",
    "\n",
    "prob_calculator_ovo.fit(X_dist_ovo_train, y_train_onehot)\n",
    "\n",
    "y_pred_probs = prob_calculator_ovo.predict_proba(X_dist_ovo_test)\n",
    "print('Test:', calc_scores(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Softmax Layer Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_prob_array, b_prob_array = prob_calculator_ovo.model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Version (with Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7087912087912088\n"
     ]
    }
   ],
   "source": [
    "# it is easier to extract the support vectors and dual coefficients with OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "model_ovo_cls = OneVsOneClassifier( SVC(kernel='rbf', C=1))\n",
    "\n",
    "model_ovo_cls.fit(X_train, y_train)\n",
    "\n",
    "print('Test Accuracy:', model_ovo_cls.score(X_test, y_test)) # should be ~0.706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def rbf(x1, x2, gamma):\n",
    "    return np.exp(-gamma * norm(x1-x2, axis=1)**2)\n",
    "\n",
    "def decision_funcion(single_model, x):\n",
    "    return ((single_model.dual_coef_\n",
    "             @ rbf(single_model.support_vectors_, x[None, :], single_model._gamma))\n",
    "            + single_model.intercept_)\n",
    "\n",
    "def generate_X_kernel_transformed(model, X):\n",
    "    kernel_transformed_X = []\n",
    "    for _, x in X_test.iterrows():\n",
    "        kernel_transformed_X.append([\n",
    "            decision_funcion(single_model, x)[0] for single_model in model.estimators_\n",
    "        ])\n",
    "    return -np.array(kernel_transformed_X)\n",
    "\n",
    "# test decision functions\n",
    "for estimator in model_ovo_cls.estimators_:\n",
    "    for _, x in X_test.iterrows():\n",
    "        np.testing.assert_almost_equal(decision_funcion(estimator, x), estimator.decision_function([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6126373626373627, 'threshold': 0.9285714285714286}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_kernel_transformed_test = generate_X_kernel_transformed(model_ovo_cls, X_test)\n",
    "y_pred_probs = prob_calculator_ovo.predict_proba(X_kernel_transformed_test)\n",
    "y_pred_avg = (y_pred_probs * np.arange(5)).sum(axis=1)\n",
    "calc_scores(y_test, y_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Version (with Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "\n",
    "# Bulding\n",
    "with g.as_default():\n",
    "\n",
    "    parameters = []\n",
    "    for index, estimator in enumerate(model_ovo_cls.estimators_):\n",
    "        name_format = 'estimator_' + str(index)\n",
    "        parameters.append({\n",
    "        'dual_coef': tf.constant(estimator.dual_coef_.squeeze(), name=name_format+'_dual_coef', dtype=tf.float32),\n",
    "        'gamma': tf.constant(estimator._gamma, name=name_format+'_gamma', dtype=tf.float32),\n",
    "        'intercept': tf.constant(estimator.intercept_.squeeze(), name=name_format+'_intercept', dtype=tf.float32),\n",
    "        'sv': tf.constant(estimator.support_vectors_, name=name_format+'_sv', dtype=tf.float32),\n",
    "    })\n",
    "    \n",
    "    \n",
    "    def tf_rbf(x1, x2, gamma):\n",
    "        return tf.exp(-gamma * tf.norm(x1-x2, axis=1)**2)\n",
    "\n",
    "    \n",
    "    def tf_decision_funcion(single_parameters, x):\n",
    "        with tf.name_scope('rbf'):\n",
    "            rbf_dist = tf_rbf(single_parameters['sv'], x[None, :], single_parameters['gamma'])\n",
    "        return (tf.tensordot(single_parameters['dual_coef'], rbf_dist, axes=1)\n",
    "                + single_parameters['intercept'])\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('redability_score') as scope:\n",
    "\n",
    "        W_prob = tf.constant(W_prob_array.T, tf.float32, name='W_prob')\n",
    "        b_prob = tf.constant(b_prob_array, tf.float32, name='b_prob')\n",
    "\n",
    "        x = tf.placeholder(tf.float32, 27, name='input_features')\n",
    "\n",
    "        svm_vals = []\n",
    "        for index, single_parameters in enumerate(parameters):\n",
    "            with tf.name_scope('decision_funcion_' + str(index)) as scope:\n",
    "                svm_vals.append(-tf_decision_funcion(single_parameters, x))\n",
    "\n",
    "        svm_vals_tensor = tf.convert_to_tensor(svm_vals, name='svm_vals')\n",
    "\n",
    "        with tf.name_scope('softmax_logits') as scope:\n",
    "            logits = tf.tensordot(W_prob, svm_vals_tensor, 1) + b_prob\n",
    "            probs = tf.nn.softmax(logits)\n",
    "\n",
    "        with tf.name_scope('mean') as scope:\n",
    "            readbility_score = tf.reduce_sum(tf.multiply(probs, np.arange(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'readability_score_tensorflow/simple/saved_model.pb'\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Initializing\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    np.testing.assert_allclose(sess.run([svm_vals], feed_dict={x: X_test.iloc[200]})[0],\n",
    "                                      X_kernel_transformed_test[200], rtol=1e-4)\n",
    "\n",
    "    np.testing.assert_allclose(sess.run([probs], feed_dict={x: X_test.iloc[200]})[0],\n",
    "                                      y_pred_probs[200], rtol=1e-4)\n",
    "\n",
    "    np.testing.assert_allclose(sess.run([readbility_score], feed_dict={x: X_test.iloc[200]})[0],\n",
    "                                      y_pred_avg[200], rtol=1e-4)\n",
    "\n",
    "\n",
    "    # Saving\n",
    "    !rm -rf readability_score_tensorflow\n",
    "\n",
    "    LOGDIR='readability_score_tensorflow'\n",
    "    train_writer = tf.summary.FileWriter(LOGDIR)\n",
    "    train_writer.add_graph(g)\n",
    "\n",
    "    tf.saved_model.simple_save(sess,\n",
    "                LOGDIR + '/simple',\n",
    "                inputs={'x': x},\n",
    "                outputs={'readbility_score': readbility_score})\n",
    "\n",
    "    !cp -rf ./readability_score_tensorflow/* /cache/tensorboard-logdir/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
