{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buidling Tenserflow version of the Readability Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and take `READABILITY_SCORES` and `POS_DENSITY` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../data')\n",
    "from corpus import load_corpus\n",
    "data = load_corpus('weebit')\n",
    "\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "y_train_onehot = data['y_train_onehot']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "y_test_onehot = data['y_test_onehot']\n",
    "\n",
    "FEATURES_NAMES = data['FEATURES_NAMES']\n",
    "features = FEATURES_NAMES['READABILITY_SCORES'] + FEATURES_NAMES['POS_DENSITY']\n",
    "\n",
    "X_train = X_train[features]\n",
    "X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def threshold_socre(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred) <= 1) / len(y_true)\n",
    "\n",
    "def calc_scores(y_true, y_pred_probs):\n",
    "    y_pred_avg = (y_pred_probs * np.arange(5)).sum(axis=1)\n",
    "    y_pred_avg_classes = y_pred_avg.round().clip(0, 4).astype(int)\n",
    "    return {'accuracy': accuracy_score(y_true, y_pred_avg_classes),\n",
    "            'threshold': threshold_socre(y_true, y_pred_avg_classes)}\n",
    "\n",
    "threshold_scorer = make_scorer(threshold_socre)\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "def calc_scores_with_cv(model, X, y, cv=5):\n",
    "    return {'accuracy': np.mean(cross_val_score(model, X, y, scoring=accuracy_scorer, cv=cv)),\n",
    "            'threshold': np.mean(cross_val_score(model, X, y, scoring=threshold_scorer, cv=cv))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Softmax layer for the SVM Probabilites Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: {'accuracy': 0.6990836779451828, 'threshold': 0.9068099410863921}\n",
      "Test: {'accuracy': 0.6181318681318682, 'threshold': 0.929945054945055}\n",
      "Test - as normal classifier accuracy: 0.7060439560439561\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_ovo = SVC(kernel='rbf', C=1, probability=True, decision_function_shape='ovo')\n",
    "\n",
    "print('CV:', calc_scores_with_cv(model_ovo, X_train, y_train))\n",
    "\n",
    "model_ovo.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_ovo.predict(X_test)\n",
    "y_pred_probs = model_ovo.predict_proba(X_test)\n",
    "print('Test:', calc_scores(y_test, y_pred_probs))\n",
    "print('Test - as normal classifier accuracy:', model_ovo.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: {'accuracy': 0.7919483530587741, 'threshold': 0.9336332073507579}\n",
      "Test: {'accuracy': 0.614010989010989, 'threshold': 0.9285714285714286}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "def create_prob_calculator_ovo():\n",
    "    prob_calculator_ovo = Sequential()\n",
    "    prob_calculator_ovo.add(Dense(5, input_dim=10, activation='softmax'))\n",
    "\n",
    "    prob_calculator_ovo.compile(optimizer='rmsprop',\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "    return prob_calculator_ovo\n",
    "\n",
    "\n",
    "prob_calculator_ovo = KerasClassifier(build_fn=create_prob_calculator_ovo,\n",
    "                                      epochs=10, \n",
    "                                      verbose=0)\n",
    "\n",
    "\n",
    "X_dist_ovo_train = model_ovo.decision_function(X_train)\n",
    "X_dist_ovo_test = model_ovo.decision_function(X_test)\n",
    "\n",
    "print('CV:', calc_scores_with_cv(prob_calculator_ovo, X_dist_ovo_train, y_train))\n",
    "\n",
    "\n",
    "prob_calculator_ovo.fit(X_dist_ovo_train, y_train_onehot)\n",
    "\n",
    "y_pred_probs = prob_calculator_ovo.predict_proba(X_dist_ovo_test)\n",
    "print('Test:', calc_scores(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Softmax Layer Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_prob_array, b_prob_array = prob_calculator_ovo.model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NumPy Version of the Readability Measure for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7087912087912088\n"
     ]
    }
   ],
   "source": [
    "# it is easier to extract the support vectors and dual coefficients with OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "model_ovo_cls = OneVsOneClassifier( SVC(kernel='rbf', C=1))\n",
    "\n",
    "model_ovo_cls.fit(X_train, y_train)\n",
    "\n",
    "print('Test Accuracy:', model_ovo_cls.score(X_test, y_test)) # should be ~0.706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def rbf(x1, x2, gamma):\n",
    "    return np.exp(-gamma * norm(x1-x2, axis=1)**2)\n",
    "\n",
    "def decision_funcion(single_model, x):\n",
    "    return ((single_model.dual_coef_\n",
    "             @ rbf(single_model.support_vectors_, x[None, :], single_model._gamma))\n",
    "            + single_model.intercept_)\n",
    "\n",
    "def generate_X_kernel_transformed(model, X):\n",
    "    kernel_transformed_X = []\n",
    "    for _, x in X_test.iterrows():\n",
    "        kernel_transformed_X.append([\n",
    "            decision_funcion(single_model, x)[0] for single_model in model.estimators_\n",
    "        ])\n",
    "    return -np.array(kernel_transformed_X)\n",
    "\n",
    "# test decision functions\n",
    "for estimator in model_ovo_cls.estimators_:\n",
    "    for _, x in X_test.iterrows():\n",
    "        np.testing.assert_almost_equal(decision_funcion(estimator, x), estimator.decision_function([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.614010989010989, 'threshold': 0.9285714285714286}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_kernel_transformed_test = generate_X_kernel_transformed(model_ovo_cls, X_test)\n",
    "y_pred_probs = prob_calculator_ovo.predict_proba(X_kernel_transformed_test)\n",
    "y_pred_avg = (y_pred_probs * np.arange(5)).sum(axis=1)\n",
    "calc_scores(y_test, y_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "\n",
    "# Bulding\n",
    "with g.as_default():\n",
    "\n",
    "    parameters = []\n",
    "    for index, estimator in enumerate(model_ovo_cls.estimators_):\n",
    "        name_format = 'estimator_' + str(index)\n",
    "        parameters.append({\n",
    "        'dual_coef': tf.constant(estimator.dual_coef_.squeeze(), name=name_format+'_dual_coef', dtype=tf.float32),\n",
    "        'gamma': tf.constant(estimator._gamma, name=name_format+'_gamma', dtype=tf.float32),\n",
    "        'intercept': tf.constant(estimator.intercept_.squeeze(), name=name_format+'_intercept', dtype=tf.float32),\n",
    "        'sv': tf.constant(estimator.support_vectors_, name=name_format+'_sv', dtype=tf.float32),\n",
    "    })\n",
    "    \n",
    "    \n",
    "    def tf_rbf(x1, x2, gamma):\n",
    "        return tf.exp(-gamma * tf.norm(x1-x2, axis=1)**2)\n",
    "\n",
    "    \n",
    "    def tf_decision_funcion(single_parameters, x):\n",
    "        with tf.name_scope('rbf'):\n",
    "            rbf_dist = tf_rbf(single_parameters['sv'], x[None, :], single_parameters['gamma'])\n",
    "        return (tf.tensordot(single_parameters['dual_coef'], rbf_dist, axes=1)\n",
    "                + single_parameters['intercept'])\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('redability_score') as scope:\n",
    "\n",
    "        W_prob = tf.constant(W_prob_array.T, tf.float32, name='W_prob')\n",
    "        b_prob = tf.constant(b_prob_array, tf.float32, name='b_prob')\n",
    "\n",
    "        x = tf.placeholder(tf.float32, 27, name='input_features')\n",
    "\n",
    "        svm_vals = []\n",
    "        for index, single_parameters in enumerate(parameters):\n",
    "            with tf.name_scope('decision_funcion_' + str(index)) as scope:\n",
    "                svm_vals.append(tf_decision_funcion(single_parameters, x))\n",
    "\n",
    "        svm_vals_tensor = tf.convert_to_tensor(svm_vals, name='svm_vals')\n",
    "\n",
    "        with tf.name_scope('softmax_logits') as scope:\n",
    "            logits = tf.tensordot(W_prob, svm_vals_tensor, 1) + b_prob\n",
    "            probs = tf.nn.softmax(logits)\n",
    "\n",
    "        with tf.name_scope('mean') as scope:\n",
    "            readbility_score = tf.reduce_sum(tf.multiply(probs, np.arange(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=0.0001, atol=0\n\n(mismatch 100.0%)\n x: array([ 1.633194,  1.76643 , -0.325358,  0.453764,  1.148039, -1.526555,\n       -0.67656 , -2.160292, -1.289705,  1.39234 ], dtype=float32)\n y: array([-1.633194, -1.76643 ,  0.325359, -0.453762, -1.148038,  1.526557,\n        0.67656 ,  2.160294,  1.289706, -1.392339])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9c18cdafacd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     np.testing.assert_allclose(sess.run([svm_vals], feed_dict={x: X_test.iloc[200]})[0],\n\u001b[0;32m---> 12\u001b[0;31m                                       X_kernel_transformed_test[200], rtol=1e-4)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     np.testing.assert_allclose(sess.run([probs], feed_dict={x: X_test.iloc[200]})[0],\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_allclose\u001b[0;34m(actual, desired, rtol, atol, equal_nan, err_msg, verbose)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Not equal to tolerance rtol=%g, atol=%g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\n\u001b[0;32m-> 1452\u001b[0;31m                          verbose=verbose, header=header, equal_nan=equal_nan)\n\u001b[0m\u001b[1;32m   1453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    787\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=0.0001, atol=0\n\n(mismatch 100.0%)\n x: array([ 1.633194,  1.76643 , -0.325358,  0.453764,  1.148039, -1.526555,\n       -0.67656 , -2.160292, -1.289705,  1.39234 ], dtype=float32)\n y: array([-1.633194, -1.76643 ,  0.325359, -0.453762, -1.148038,  1.526557,\n        0.67656 ,  2.160294,  1.289706, -1.392339])"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Initializing\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    np.testing.assert_allclose(sess.run([svm_vals], feed_dict={x: X_test.iloc[200]})[0],\n",
    "                                      X_kernel_transformed_test[200], rtol=1e-4)\n",
    "\n",
    "    np.testing.assert_allclose(sess.run([probs], feed_dict={x: X_test.iloc[200]})[0],\n",
    "                                      y_pred_probs[200], rtol=1e-4)\n",
    "\n",
    "    np.testing.assert_allclose(sess.run([readbility_score], feed_dict={x: X_test.iloc[200]})[0],\n",
    "                                      y_pred_avg[200], rtol=1e-4)\n",
    "\n",
    "\n",
    "    # Saving\n",
    "    !rm -rf readability_score_tensorflow\n",
    "\n",
    "    LOGDIR='readability_score_tensorflow'\n",
    "    train_writer = tf.summary.FileWriter(LOGDIR)\n",
    "    train_writer.add_graph(g)\n",
    "\n",
    "    tf.saved_model.simple_save(sess,\n",
    "                LOGDIR + '/simple',\n",
    "                inputs={'x': x},\n",
    "                outputs={'readbility_score': readbility_score})\n",
    "\n",
    "    !cp -rf ./readability_score_tensorflow/* /cache/tensorboard-logdir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
