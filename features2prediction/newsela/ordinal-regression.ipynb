{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../data')\n",
    "from corpus import load_corpus\n",
    "data = load_corpus('newsela')\n",
    "\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "y_train_onehot = data['y_train_onehot']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "y_test_onehot = data['y_test_onehot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[y_train <= 8]\n",
    "y_train = y_train[y_train <= 8]\n",
    "\n",
    "X_test = X_test[y_test <= 8]\n",
    "y_test = y_test[y_test <= 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mord import LogisticAT, LogisticIT, LAD, OrdinalRidge \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def train_ordinal_regression_model(model_class, X_train, y_train, model_params={}, hyperparameters_grid={}):\n",
    "    \n",
    "    name = model_class.__name__\n",
    "\n",
    "    try:\n",
    "        model = model_class(random_state=42, **model_params)\n",
    "    except TypeError:\n",
    "        model = model_class(**model_params)\n",
    "\n",
    "    if hyperparameters_grid:\n",
    "        model = GridSearchCV(model, hyperparameters_grid)\n",
    "        # name += ' GridCV'\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return name, model\n",
    "\n",
    "\n",
    "ALPHAs = 10.**np.arange(-4, 5)\n",
    "Cs = 10.**np.arange(-4, 5)\n",
    "\n",
    "models = dict([train_ordinal_regression_model(model_class, X_train, y_train, model_params, hyperparameters_grid)\n",
    "          for model_class, model_params, hyperparameters_grid in\n",
    "                                     tqdm([\n",
    "                                         (LogisticAT, {},  {'alpha': ALPHAs}),\n",
    "                                         (LogisticIT, {},  {'alpha': ALPHAs}),\n",
    "                                         (LAD, {}, {'C': Cs}),\n",
    "                                         (OrdinalRidge, {}, {'alpha': ALPHAs}),\n",
    "                                     ])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - TODO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://ttic.uchicago.edu/~nati/Publications/RennieSrebroIJCAI05.pdf\n",
    "* https://arxiv.org/abs/0704.1028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score, r2_score,\\\n",
    "                            classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "\n",
    "def threshold_socre(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred) <= 1) / len(y_true)\n",
    "\n",
    "def f1_score_micro(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "   \n",
    "def calc_metrics(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    \n",
    "    y_pred_classes = np.clip(y_pred.round().astype(int), 0, 4)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    metrics.update({metrics.__name__: metrics(y_test, y_pred) for metrics in [mean_absolute_error,\n",
    "                                                                      mean_squared_error,\n",
    "                                                                      explained_variance_score,                                                                      explained_variance_score,\n",
    "                                                                      r2_score\n",
    "    ]})\n",
    "    \n",
    "    metrics.update({metrics.__name__: metrics(y_test, y_pred_classes) for metrics in [accuracy_score,\n",
    "                                                              classification_report,\n",
    "                                                              confusion_matrix,\n",
    "                                                              f1_score_micro,\n",
    "                                                              threshold_socre,\n",
    "    ]})\n",
    "\n",
    "    return metrics\n",
    "\n",
    "evaluation_df = pd.DataFrame({name: calc_metrics(model, X_test, y_test) for name, model in models.items()}).transpose()\n",
    "evaluation_df = evaluation_df.sort_values('threshold_socre', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_evaluation_df = (evaluation_df[['accuracy_score', 'f1_score_micro', 'threshold_socre', 'mean_absolute_error']]\n",
    "     .astype(float).round(4))\n",
    "\n",
    "summary_evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_evaluation_df.to_excel('ordinal_regression_evaluation.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "axes = itertools.chain(*axes)\n",
    "for (name, r), ax in zip(evaluation_df.iterrows(), axes):\n",
    "    cm = r['confusion_matrix']\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm, vmin=0, vmax=1, annot=True,\n",
    "                ax=ax)\n",
    "    ax.set_title(name + ', acc={:0.3f}, th={:0.4f}'.format(r['accuracy_score'], r['threshold_socre']))\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "f.text(0.04, 0.5, 'True y', va='center', rotation='vertical', fontsize=30)\n",
    "f.suptitle('Normalized Confusion Matrices', fontsize=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probs Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "#axes = itertools.chain(*axes)\n",
    "for (name, r), ax in zip(evaluation_df[evaluation_df.index.isin(['LogisticAT', 'LogisticIT'])].iterrows(), axes):\n",
    "\n",
    "    model = models[name]\n",
    "    try:\n",
    "        y_pred_probs = model.predict_proba(X_test)\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    \n",
    "    assert np.allclose(y_pred_probs.sum(axis=1), 1)\n",
    "    \n",
    "    sns.distplot(np.choose(y_test, y_pred_probs.T), label='hit', ax=ax)\n",
    "\n",
    "\n",
    "    sns.distplot(np.array([[prob for index, prob in enumerate(probs) \n",
    "                      if cls != index ]\n",
    "                 for probs, cls in  zip(y_pred_probs, y_test)]).flatten(),\n",
    "             label='miss', ax=ax)\n",
    "\n",
    "    ax.set_title(name + ', acc={:0.3f}, th={:0.4f}'.format(r['accuracy_score'], r['threshold_socre']))\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "f.suptitle('Dist hit/miss probs', fontsize=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average y as Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score, r2_score,\\\n",
    "                            classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "\n",
    "def threshold_socre(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred) <= 1) / len(y_true)\n",
    "\n",
    "def f1_score_micro(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "   \n",
    "def calc_metrics(model, X_test, y_test):\n",
    "\n",
    "    y_pred_probs = model.predict_proba(X_test)\n",
    "    y_pred_avg = (y_pred_probs * np.arange(5)).sum(axis=1)\n",
    "    y_pred_avg_classes = y_pred_avg.round().clip(0, 4).astype(int)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    metrics.update({metrics.__name__: metrics(y_test, y_pred_avg) for metrics in [mean_absolute_error,\n",
    "                                                                      mean_squared_error,\n",
    "                                                                      explained_variance_score,                                                                      explained_variance_score,\n",
    "                                                                      r2_score\n",
    "                                                                      \n",
    "    ]})\n",
    "    \n",
    "    metrics.update({metrics.__name__: metrics(y_test, y_pred_avg_classes) for metrics in [accuracy_score,\n",
    "                                                              classification_report,\n",
    "                                                              confusion_matrix,\n",
    "                                                              f1_score_micro,\n",
    "                                                              threshold_socre,\n",
    "    ]})\n",
    "\n",
    "    return metrics\n",
    "\n",
    "evaluation_df = pd.DataFrame({name: calc_metrics(model, X_test, y_test) for name, model in models.items()\n",
    "                             if name in ['LogisticAT', 'LogisticIT']}).transpose()\n",
    "evaluation_df = evaluation_df.sort_values('threshold_socre', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_evaluation_df = (evaluation_df[['accuracy_score', 'f1_score_micro', 'threshold_socre', 'mean_squared_error', 'mean_absolute_error', 'explained_variance_score', 'r2_score']]\n",
    "     .astype(float).round(4))\n",
    "summary_evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_evaluation_df.to_excel('probs_avg_ordinal_regression_evaluation.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
