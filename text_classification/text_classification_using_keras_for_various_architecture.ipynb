{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDate : 14/06/2018\\nTime : 10:35\\nVersion : 1.1\\nDataset : IMDB dataset(https://www.kaggle.com/c/word2vec-nlp-tutorial/data)\\n          Glove 6B(https://nlp.stanford.edu/projects/glove/)\\nDescription : Performing Text Classification using Keras for different deep neural architectures.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Date : 14/06/2018\n",
    "Time : 10:35\n",
    "Version : 1.1\n",
    "Dataset : IMDB dataset(https://www.kaggle.com/c/word2vec-nlp-tutorial/data)\n",
    "          Glove 6B(https://nlp.stanford.edu/projects/glove/)\n",
    "Description : Performing Text Classification using Keras for different deep neural architectures.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n! pip install numpy\\n! pip install pandas\\n! apt-get install python3-bs4 \\n! easy_install beautifulsoup4\\n! pip install beautifulsoup4\\n! python setup.py install\\n! pip install keras\\n! pip install theano; python -c \"import theano\"\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Installing dependencies\n",
    "\"\"\"\n",
    "! pip install numpy\n",
    "! pip install pandas\n",
    "! apt-get install python3-bs4 \n",
    "! easy_install beautifulsoup4\n",
    "! pip install beautifulsoup4\n",
    "! python setup.py install\n",
    "! pip install keras\n",
    "! pip install theano; python -c \"import theano\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import types\n",
    "import tempfile\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.layers import LSTM, GRU, Bidirectional\n",
    "from keras.models import Model,load_model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using CNN (https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the hyper-parameters\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "SPLIT_RATIO = 0.25\n",
    "DROPOUT_MARGIN = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BeautifulSoup to remove some html tags and remove some unwanted characters.\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training dataset: (25000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the training data\n",
    "train_data = pd.read_csv(\"labeledTrainData.tsv\",sep=\"\\t\")\n",
    "print(\"Shape of the training dataset:\",train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the valuesof texts and labels as per the features and labels of your dataset \n",
    "texts,labels = ([] for i in range(2))\n",
    "for idx in range(train_data.review.shape[0]):\n",
    "    text = BeautifulSoup(train_data.review[idx])\n",
    "    texts.append(clean_str(text.get_text()))\n",
    "    labels.append(train_data.sentiment[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 81501 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Performing text-processing\n",
    "# Initializing Tokenizer from keras\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "# Fitting text on the tokenizer\n",
    "tokenizer.fit_on_texts(texts)\n",
    "# Converting text to sequence\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Finding unique tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Padding sequences to the length of MAX_SEQUENCE_LENGTH\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 1000)\n",
      "Shape of label tensor: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Getting the labels and features data\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the splitting index for training and testing data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "validation_samples = int(SPLIT_RATIO * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the testing and training dataset\n",
    "X_train = data[:-validation_samples]\n",
    "y_train = labels[:-validation_samples]\n",
    "X_test = data[-validation_samples:]\n",
    "y_test = labels[-validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "# Using Google Glove 6B vector 100d and randomizing the vector\n",
    "\"\"\"\n",
    "Glove is an unsupervised learning algorithm for obtaining vector representations for words. \n",
    "Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting \n",
    "representations showcase interesting linear substructures of the word vector space.\n",
    "\"\"\"\n",
    "current_directory = os.getcwd()\n",
    "glove_directory = current_directory + \"/glove.6B/glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "f = open(glove_directory)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting vector for unique words in the corpus\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Initializing the embedded layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH,trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Simple Convolution Neural Network (Accuracy = 96.82%)\n",
    "# (Filters = 128, Size = 5, and Maximum Pooling of 5 and 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Configuring the neural network\\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\\nembedded_sequences = embedding_layer(sequence_input)\\nl_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\\nl_pool1 = MaxPooling1D(5)(l_cov1)\\nl_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\\nl_pool2 = MaxPooling1D(5)(l_cov2)\\nl_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\\nl_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\\nl_flat = Flatten()(l_pool3)\\nl_dense = Dense(128, activation='relu')(l_flat)\\npreds = Dense(2, activation='softmax')(l_dense)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "\"\"\"\n",
    "# Configuring the neural network\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Compiling the model for cross entropy loss\\nmodel_simple_cnn = Model(sequence_input, preds)\\nmodel_simple_cnn.compile(loss=\\'categorical_crossentropy\\',optimizer=\\'rmsprop\\',metrics=[\\'acc\\'])\\nprint(\"model fitting - simplified convolutional neural network\")\\nmodel_simple_cnn.summary()\\n# Training the model\\nmodel_simple_cnn.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=10, batch_size=128)\\n# Saving the model\\nmodel_simple_cnn.save(\"simple_cnn.h5\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Compiling the model for cross entropy loss\n",
    "model_simple_cnn = Model(sequence_input, preds)\n",
    "model_simple_cnn.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "print(\"model fitting - simplified convolutional neural network\")\n",
    "model_simple_cnn.summary()\n",
    "# Training the model\n",
    "model_simple_cnn.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=10, batch_size=128)\n",
    "# Saving the model\n",
    "model_simple_cnn.save(\"simple_cnn.h5\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_simple_cnn = load_model(\"simple_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.82%\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy\n",
    "accuracy_simple_cnn = model_simple_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_simple_cnn[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_simple_cnn = model_simple_cnn.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deeper Convolutional neural network with multiple filter sizes (Accuracy = 97.87%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nconvs = []\\nfilter_sizes = [3,4,5]\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "\"\"\"\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Configuring the neural network\\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\\nembedded_sequences = embedding_layer(sequence_input)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Configuring the neural network\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor fsz in filter_sizes:\\n    l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\\n    l_pool = MaxPooling1D(5)(l_conv)\\n    convs.append(l_pool)\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nl_merge = Merge(mode='concat', concat_axis=1)(convs)\\nl_cov1= Conv1D(128, 5, activation='relu')(l_merge)\\nl_pool1 = MaxPooling1D(5)(l_cov1)\\nl_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\\nl_pool2 = MaxPooling1D(30)(l_cov2)\\nl_flat = Flatten()(l_pool2)\\nl_dense = Dense(128, activation='relu')(l_flat)\\npreds = Dense(2, activation='softmax')(l_dense)\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(30)(l_cov3)\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Compiling the Model for cross entropy loss\\nmodel_deep_cnn = Model(sequence_input, preds)\\nmodel_deep_cnn.compile(loss=\\'categorical_crossentropy\\',\\n              optimizer=\\'rmsprop\\',\\n              metrics=[\\'acc\\'])\\n\\nprint(\"model fitting - more complex convolutional neural network\")\\nmodel_deep_cnn.summary()\\n# Training the model\\nmodel_deep_cnn.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=20, batch_size=50)\\n# Saving the model\\nmodel_deep_cnn.save(\"deep_cnn.h5\")\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Compiling the Model for cross entropy loss\n",
    "model_deep_cnn = Model(sequence_input, preds)\n",
    "model_deep_cnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - more complex convolutional neural network\")\n",
    "model_deep_cnn.summary()\n",
    "# Training the model\n",
    "model_deep_cnn.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=20, batch_size=50)\n",
    "# Saving the model\n",
    "model_deep_cnn.save(\"deep_cnn.h5\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_deep_cnn = load_model(\"deep_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.87%\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy\n",
    "accuracy_deep_cnn = model_deep_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_deep_cnn[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_deep_cnn = model_deep_cnn.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using Bidirectional LSTM encoders without adding dropout(Accuracy = 96.82%)\n",
    "# (https://arxiv.org/pdf/1801.06261.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBy using LSTM encoder, we intent to encode all information of the text in the last output of recurrent neural \\nnetwork. Bidirectional LSTM is used in the below mentioned code and the last output of both LSTM are \\nconcatenated at the end.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "By using LSTM encoder, we intent to encode all information of the text in the last output of recurrent neural \n",
    "network. Bidirectional LSTM is used in the below mentioned code and the last output of both LSTM are \n",
    "concatenated at the end.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Configuring the neural network\\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\\'int32\\')\\nembedded_sequences = embedding_layer(sequence_input)\\nl_lstm = Bidirectional(LSTM(100))(embedded_sequences)\\npreds = Dense(2, activation=\\'softmax\\')(l_lstm)\\nmodel_lstm = Model(sequence_input, preds)\\nmodel_lstm.compile(loss=\\'categorical_crossentropy\\',optimizer=\\'rmsprop\\',metrics=[\\'acc\\'])\\n\\nprint(\"model fitting - Bidirectional LSTM\")\\nmodel_lstm.summary()\\nmodel_lstm.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=10, batch_size=50)\\n# Saving the model\\nmodel_lstm.save(\"lstm_encoder.h5\")\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "\n",
    "\"\"\"\n",
    "# Configuring the neural network\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "preds = Dense(2, activation='softmax')(l_lstm)\n",
    "model_bidirectional_lstm = Model(sequence_input, preds)\n",
    "model_bidirectional_lstm.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Bidirectional LSTM\")\n",
    "model_bidirectional_lstm.summary()\n",
    "model_bidirectional_lstm.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=10, batch_size=50)\n",
    "# Saving the model\n",
    "model_bidirectional_lstm.save(\"lstm_encoder_without_dropout_layer.h5\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_bidirectional_lstm = load_model(\"simple_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.82%\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy\n",
    "accuracy_bidirection_lstm = model_bidirectional_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_bidirection_lstm[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_bidirectional_lstm = model_bidirectional_lstm.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using LSTM with dropout(Accuracy = 88.02%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\\'int32\\')\\nembedded_sequences = embedding_layer(sequence_input)\\nmodel_lstm_with_dropout = Sequential()\\nmodel_lstm_with_dropout.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length = MAX_SEQUENCE_LENGTH))\\nmodel_lstm_with_dropout.add(LSTM(100,dropout=DROPOUT_MARGIN, recurrent_dropout=DROPOUT_MARGIN))\\nmodel_lstm_with_dropout.add(Dense(2, activation=\\'softmax\\'))\\nmodel_lstm_with_dropout.compile(loss=\\'categorical_crossentropy\\',optimizer=\\'rmsprop\\',metrics=[\\'acc\\'])\\n\\nprint(\"model fitting - LSTM with dropout\")\\nmodel_lstm_with_dropout.summary()\\nmodel_lstm_with_dropout.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=10, batch_size=50)\\n# Saving the model\\nmodel_lstm_with_dropout.save(\"lstm_encoder_with_dropout_layer.h5\")\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "# dropout is applied to the input and recurrent connections of the memory units with the LSTM precisely and \n",
    "# separately.\n",
    "\n",
    "# Configuring the neural network\n",
    "\"\"\"\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "model_lstm_with_dropout = Sequential()\n",
    "model_lstm_with_dropout.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length = MAX_SEQUENCE_LENGTH))\n",
    "model_lstm_with_dropout.add(LSTM(100,dropout=DROPOUT_MARGIN, recurrent_dropout=DROPOUT_MARGIN))\n",
    "model_lstm_with_dropout.add(Dense(2, activation='softmax'))\n",
    "model_lstm_with_dropout.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - LSTM with dropout\")\n",
    "model_lstm_with_dropout.summary()\n",
    "model_lstm_with_dropout.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=10, batch_size=50)\n",
    "# Saving the model\n",
    "model_lstm_with_dropout.save(\"lstm_encoder_with_dropout_layer.h5\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_lstm_with_dropout = load_model(\"lstm_encoder_with_dropout_layer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.02%\n"
     ]
    }
   ],
   "source": [
    "# checking the accuracy\n",
    "accuracy_lstm_with_dropout = model_lstm_with_dropout.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_lstm_with_dropout[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_lstm_with_dropout = model_lstm_with_dropout.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using CNN and LSTM on the top of each other with dropout(http://www.aclweb.org/anthology/C16-1258, Accuracy = 99.40%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdding an one-dimensional CNN and max pooling layers after the Embedding layer which is then feed the \\nconsolidated features to the LSTM.\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adding an one-dimensional CNN and max pooling layers after the Embedding layer which is then feed the \n",
    "consolidated features to the LSTM.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Configuring the neural network\\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\\'int32\\')\\nembedded_sequences = embedding_layer(sequence_input)\\nmodel_cnn_lstm = Sequential()\\nmodel_cnn_lstm.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length = MAX_SEQUENCE_LENGTH))\\nmodel_cnn_lstm.add(Conv1D(filters=128, kernel_size=5, padding=\\'same\\', activation=\\'relu\\'))\\nmodel_cnn_lstm.add(MaxPooling1D(pool_size=2))\\nmodel_cnn_lstm.add(LSTM(100,dropout=DROPOUT_MARGIN, recurrent_dropout=DROPOUT_MARGIN))\\n\\nmodel_cnn_lstm.add(Dense(2, activation=\\'softmax\\'))\\nmodel_cnn_lstm.compile(loss=\\'categorical_crossentropy\\',optimizer=\\'rmsprop\\',metrics=[\\'acc\\'])\\n\\nprint(\"model fitting - CNN and LSTM\")\\nmodel_cnn_lstm.summary()\\nmodel_cnn_lstm.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=10, batch_size=50)\\n# Saving the model\\nmodel_cnn_lstm.save(\"lstm_cnn.h5\")\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "\n",
    "\"\"\"\n",
    "# Configuring the neural network\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length = MAX_SEQUENCE_LENGTH))\n",
    "model_cnn_lstm.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "model_cnn_lstm.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn_lstm.add(LSTM(100,dropout=DROPOUT_MARGIN, recurrent_dropout=DROPOUT_MARGIN))\n",
    "\n",
    "model_cnn_lstm.add(Dense(2, activation='softmax'))\n",
    "model_cnn_lstm.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - CNN and LSTM\")\n",
    "model_cnn_lstm.summary()\n",
    "model_cnn_lstm.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=10, batch_size=50)\n",
    "# Saving the model\n",
    "model_cnn_lstm.save(\"lstm_cnn.h5\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_cnn_lstm = load_model(\"lstm_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.11%\n"
     ]
    }
   ],
   "source": [
    "# checking the accuracy\n",
    "accuracy_lstm_cnn = model_cnn_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_lstm_cnn[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_lstm_cnn = model_cnn_lstm.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using Attentional based GRU network\n",
    "# (http://colinraffel.com/publications/iclr2016feed.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe below mentioned code illustrates a simplified model of attention which is applicable to feed-forward neural\\nnetworks and demonstrate that the resulting model can solve the synthetic “addition” and “multiplication” \\nlong-term memory problems for longer sequence lengths.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The below mentioned code illustrates a simplified model of attention which is applicable to feed-forward neural\n",
    "networks and demonstrate that the resulting model can solve the synthetic “addition” and “multiplication” \n",
    "long-term memory problems for longer sequence lengths.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a custom Keras layer to implement attention layer.\n",
    "\"\"\"\n",
    "The following code can only strictly run on Theano backend since tensorflow matrix dot product doesn’t behave \n",
    "the same as np.dot. I don’t know how to get a 2D tensor by dot product of 3D tensor of recurrent layer output \n",
    "and 1D tensor of weight.\n",
    "\"\"\"\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\\'int32\\')\\nembedded_sequences = embedding_layer(sequence_input)\\nl_gru = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\\nl_att = AttLayer()(l_gru)\\npreds = Dense(2, activation=\\'softmax\\')(l_att)\\nmodel_attension_gru = Model(sequence_input, preds)\\nmodel_attension_gru.compile(loss=\\'categorical_crossentropy\\',optimizer=\\'rmsprop\\',metrics=[\\'acc\\'])\\n\\nprint(\"model fitting - attention GRU network\")\\nmodel_attension_gru.summary()\\nmodel_attension_gru.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=10, batch_size=50)\\n# Saving the model\\nmodel_attension_gru.save(\"attension-gru.h5\")'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-comment the below mentioned code to train your model\n",
    "\n",
    "# Configuring the neural network\n",
    "\"\"\"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_gru = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer()(l_gru)\n",
    "preds = Dense(2, activation='softmax')(l_att)\n",
    "model_attension_gru = Model(sequence_input, preds)\n",
    "model_attension_gru.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - attention GRU network\")\n",
    "model_attension_gru.summary()\n",
    "model_attension_gru.fit(X_train, y_train, validation_data=(X_test, y_test),nb_epoch=10, batch_size=50)\n",
    "# Saving the model\n",
    "model_attension_gru.save(\"attension-gru.h5\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pre-trained model\n",
    "model_attension_gru = load_model(\"attension-gru.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions in terms of probabilities for each class\n",
    "prediction_attension_gru = model_attension_gru.predict(X_test,batch_size=10,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Hierarchical attention network\n",
    "# (https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the data input as 3D numpy arrays\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            #update 1/10/2017 - bug fixed - set max number of words\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    data[i,j,k] = tokenizer.word_index[word]\n",
    "                    k=k+1               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
